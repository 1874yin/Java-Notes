## 雪花算法

单体项目中一个表中的主键 ID 都是自增的，MySQL 利用 autoincrement 实现，Oracle 利用序列实现。单表数据量上来以后就要进行水平分表（阿里Java开发建议单表大于 500w 进行分表），一张表拆分成多张表，如果按照以前的自增来做主键 ID，就会出现 ID 重复。所以就需要分布式 ID。

分布式 ID 必须具备的特点：

- 高性能
- 单调递增
- 全局唯一

### 方案

1. 数据库表

   在某个库专门维护一张表，无论哪个表需要自增 ID，都去查这个表的记录，用 `for update` 锁表，取到的值 +1，返回以后再把值记录到表中。该方法仅适用于并发量较小的项目。

2. Redis

   Redis 是单线程，可以在 Redis 中维护一个键值对，然后哪个表需要直接去 Redis 中取值然后 +1。

   由于是单线程，对高并发的支持不高，只适合并发量小的项目。

3. UUID

   使用 UUID 作为不重复的主键 ID，但是 UUID 是无序的字符串，会导致主键索引失效。另外 UUID 使用字符存储，查询效率低，占用空间大。

4. 雪花算法

   使用 64 位 long 类型的数据存储 ID。

   - 最高一位是符号位，一般是正数，也就是0
   - 41位存储毫秒级时间戳，`System.currentTimeMillis()` 生成的时间转换为二进制刚好41位
   - 10位存储机器码（包括 5位 dataCenterId，5位 workerId）
   - 12位存储序列号 sequence，序列号自增

   所以，`1+41+10+12=64` 刚好是 long 类型的大小。

   ![img](https://upload-images.jianshu.io/upload_images/13382703-b64e38457ddd13e2.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1021/format/webp)

### 实现

```java
public class SnowFlake {
    /**
     * 时间戳起始位置
     */
    private final static long TIMESTAMP_START = 1709603659000L; //  2024.0305 09:54
    /**
     * 各部分占用比特数
     */
    private final static long DATA_CENTER_BIT = 5L;		// 数据中心
    private final static long WORKER_BIT = 5L;			// worker
    private final static long SEQUENCE_BIT = 12L;		// 序号
    /**
     * 各部分最大值
     */
    private final static long MAX_DATA_CENTER = ~(-1 << DATA_CENTER_BIT);
    private final static long MAX_WORKER = ~(-1 << WORKER_BIT);
    private final static long MAX_SEQUENCE = ~(-1 << SEQUENCE_BIT);
    /**
     * 各部分左移位数
     */
    private final static long WORKER_LEFT = SEQUENCE_BIT;
    private final static long DATA_CENTER_LEFT = WORKER_LEFT + WORKER_BIT;
    private final static long TIMESTAMP_LEFT = DATA_CENTER_LEFT + DATA_CENTER_BIT; 
    /**
     * 各种初始值
     */
    private final long dataCenterId;
    private final long workerId;
    
    private long sequence = 0L;
    private long last_timestamp = -1L;
    
    public SnowFlake(long dataCenterId, long workerId) {
        if (dataCenterId > MAX_DATA_CENTER || dataCenterId < 0) {
            throw new IllegalArgumentException(
            		String.format("DataCenterId can't be greater than %d or less than 0. DataCenterId: %d",
                    	MAX_DATA_CENTER, dataCenterId));
        }
        if (workerId > MAX_WORKER || workerId < 0) {
            throw new IllegalArgumentException(
                    String.format("WorkerId can't be greater than %d or less than 0. WorkerId: %d",
                            MAX_WORKER, workerId));
        }
        this.dataCenterId = dataCenterId;
        this.workerId = workerId;
    }
    
    /**
     * 获取当前时间戳
     */
    private long getNowTime() {
        return System.currentTimeMillis();
    }
    
    /**
     * 获取下一个时间戳
     */
    private long getNextTime() {
        long nowTime = getNowTime();
        while (nowTime < lastTimestamp) {
            nowTime = getNowTime();
        }
        return nowTime;
    }
    
    /**
     * 生成 ID
     */
    prulic synchronized long nextId() {
        long currentTimestamp = getNextTime();
        if (currentTimestamp < this.lastTimestamp) {
            throw new RuntimeException("System clock move backward. Refuse to generate ID");
        }
        if (currentTimestamp == this.lastTimestamp) {
            this.sequence = (this.sequence + 1) & MAX_SEQUENCE;
            if (this.sequence == 0L) {
                currentTimestamp = getNextTime();
            }
        } else {
            this.sequence = 0L;
        }
        this.lastTimestamp = currentTimestamp;
        
        return (currentTimestamp - TIMESTAMP_START) << TIMESTAMP_LEFT
            	| this.dataCenterId << DATA_CENTER_LEFT
            	| this.workerId << WORKER_LEFT
            	| this.sequence;
    }
    
    public static void main(String[] args) {
        SnowFlake sf = new SnowFlake(2, 6);
        for (int i = 0; i < ~(-1L << 5); i++) {
            System.out.println(sf.nextId());
        }
    }

```

#### 输出

```shell
32721841971200
32721841971201
32721841971202
32721841971203
32721841971204
32721841971205
32721841971206
32721841971207
32721841971208
32721841971209
32721841971210
32721841971211
32721841971212
32721841971213
32721841971214
32721841971215
32721841971216
32721841971217
32721841971218
32721841971219
32721841971220
32721841971221
32721841971222
32721841971223
32721841971224
32721841971225
32721841971226
32721841971227
32721841971228
32721841971229
32721841971230

Process finished with exit code 0

```



雪花算法可以在不同机器上生成不重复递增 ID，需要防止服务器时间回拨导致的 ID 生成重复。

- 41 bit 可以表示 2 ^ 41 - 1 个数字，转换为单位年则是 `(2^41) / (1000 * 60 * 60 * 24 * 365) = 69 年
- 10 bit 的工作机器 ID ，用来记录工作机器的 ID。可以部署在 2 ^ 10 = 1024 个节点上。2 ^ 5 - 1 = 31，即可以用 0-31 的数字来表示 dataCenterId 和 workerId
- 12 bit 序列号用来记录同一毫秒内产生的不同 ID。2 ^ 12 - 1 = 4095，同一机器同一毫秒内可以产生 4095 个 ID 序号

#### NOTE

1. `~(-1 << 5)` 用来计算 5 位比特所能表示的最大数字，等同于 `2 ^ 5 -1` 
2. 任何数 `n <= m`，`n & m` 的结果都等于 n 。`(m + 1) & m `  结果等于 0。`&` 是按位与操作符，在二进制位上，同位如果都为1，则该位结果为1，否则为0。如 `110011 & 100010 = 100010` 
3. 加上 `TIMESTAMP_START` 来减小时间戳起点，可以增加该算法可使用年限
4. 使用 `<<` 左移操作符，在 64 位的二进制位上移动后，后方的比特位便留空了（都为0），这时候使用 `|` 操作符，效果相当于拼接二进制。

## 补充：Leaf——美团点评分布式ID生成系统

业务系统对 ID 号的要求：

1. 全局唯一性：唯一标识的基本要求。
2. 趋势递增：多数 RDBMS 使用 B-tree 的数据结构来存储索引数据，主键应该尽量使用有序主键保证写入性能。
3. 单调递增：保证下一个 ID 一定大于上一个 ID，例如事务版本号、IM 增量消息、排序等特殊需求。
4. 信息安全：如果 ID 是连续的，容易被恶意爬取，直接按照顺序下载指定 URL 即可；如果是订单号，还会被猜出一天订单数量。在一些应用场景下，需要 ID 无规则、不规则。

对 ID 号生成系统的可用性要求：

1. 平均延迟和 TP999 延迟都要尽可能低
2. 可用性5个9
3. 高 QPS

### 常见方法介绍

#### UUID

包含32个15进制数字，以连字号分为五段，形式为 8-4-4-4-12 的36个字符。

优点：

- 性能非常高：本地生成，没有网络消耗

缺点：

- 不易于存储：UUID 太长，16字节128位
- 信息不安全：基于 MAC 地址生成 UUID 的算法可能会造成 MAC 地址泄露
- ID 作为主键在特定的环境会存在一些问题，比如做 DB 主键就非常不适用
  1. MySQL 官方明确建议主键要尽量越短越好
  2. 对 MySQL 索引不利：如果作为数据库主键，UUID 的无序性会引起数据位置频繁变动，严重影响性能

#### 类 SnowFlake 方案

将 64-bit 分别划分成多段，分开来标识机器、时间等。

优点：

- 毫秒数在高位，自增序列在低位，保证了趋势递增
- 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高
- 可以根据自身业务特性分配 bit 位，非常灵活

缺点：

- 强依赖机器时钟，如果机器始终回拨，会导致发号重复或者服务处于不可用状态

#### 数据库生成

以 MySQL 为例，给字段设置 `auto_increment_increment` (增量，也就是增长步长 step) 和 `auto_increment_offset` (初始值，增长起点) 来保证 ID 自增，通过下列 SQL 读写得到 ID 号：

```mysql
begin;
REPLACE INFO Tickets64 (stub) VALUES ('a');
SELECT LAST_INSERT_ID();
commit;
```

优点：

- 非常简单，利用现有数据库系统的功能实现，成本小，有 DBA 专业维护
- ID 号单调自增，可以实现一些对 ID 有特殊要求的业务

缺点：

- 强依赖 DB，当 DB 异常时，整个系统不可用，属于致命问题。配置主从复制可以尽可能增加可用性，但是难以保证主从切换时的数据一致性
- ID 发号性能瓶颈限制在单台 MySQL 的读写性能

水平扩展：多部署几台机器，每台机器设置不一样的起始值，步长 step 和机器数相等。比如三台机器，设置 step 为3

TicketServer1 生成的 ID 为

```
1, 4, 7, 10, 13...
```

TicketServer2 生成的 ID 为

```
2, 5, 8, 11, 14...
```

TicketServer3 生成的 ID 为

```
3, 6, 9, 12, 15...
```

缺点是如果需要进行水平扩展比较困难；ID 没有了单调递增的特性，只能趋势递增；每次获取 ID 都得读写一次数据库，数据库压力还是很大。

#### Leaf 方案实现

##### Leaf-segment数据库方案

在使用数据库方案上，做了如下改变：

- 将每次获取 ID 都得读写一次数据库，改为利用 proxy-server 批量获取，每次获取一个 segment 号段的值，用完之后再去数据库获取新的号段，大大减轻数据库压力
- 各个业务不同的发号需求用 biz_tag 字段来区分，每个 biz_tag 的 ID 获取相互隔离，互不影响。如果日后有扩容需求，只需要对 biz_tag 分库分表

数据库表设计如下：

```mysql
+-------------+--------------+------+-----+-------------------+-----------------------------+
| Field       | Type         | Null | Key | Default           | Extra                       |
+-------------+--------------+------+-----+-------------------+-----------------------------+
| biz_tag     | varchar(128) | NO   | PRI |                   |                             |
| max_id      | bigint(20)   | NO   |     | 1                 |                             |
| step        | int(11)      | NO   |     | NULL              |                             |
| desc        | varchar(256) | YES  |     | NULL              |                             |
| update_time | timestamp    | NO   |     | CURRENT_TIMESTAMP | on update CURRENT_TIMESTAMP |
+-------------+--------------+------+-----+-------------------+-----------------------------+
```

- biz_tag 用来区分业务
- max_id 表示当前所被分配的 ID 号段的最大值
- step 表示每次分配号段的长度

原来获取 ID 每次都需要请求数据库，现在只要把 step 设置得足够大，只有当号段内的 ID 被用完，才会重新读取一次数据库，读写频率降低到 1/step

现在假设数据库中一条记录为

```mysql
+----------+------+----+------+------------+
| biz_tag  |max_id|step| desc | update_time| 
+----------+-----------+------+------------+
| test_tag | 3001 |1000|      |            |
+----------+------+----+------+------------+
```

test_tag 在第一台机器上是 1~1000 的号段，假设另外两台号段都没有更新，这个时候第一台机器新加载的号段应该为 3001~4000。同时数据库中 test_tag 对应的 max_id 应该被更新为 4001，更新的 SQL 语句如下：

```mysql
Begin
UPDATE table SET max_id = max_id+step WHERE biz_tag = 'test_tag'
SELECT biz_tag, max_id, step FROM table WHERE biz_tag = 'test_tag'
Commit
```

优点：

- Leaf 服务可以很方便地线性扩展
- ID 号码趋势递增
- 容灾性高：Leaf 内部有号段缓存，即使 DB 宕机，短时间内也能对外提供服务
- 可以自定义 max_id 大小，方便迁移

缺点：

- ID 号码不够随机，能够泄露发号数量，不太安全
- TP999数据波动大，当号码使用完后会 hang 在更新数据库的 I/O 上
- DB 宕机会造成整个系统不可用

**双 Buffer 优化**

对于第二个缺点，Leaf 是在号段消耗完的时候去取新号段，也就是说 ID 到达临界点时，获取新 ID 的时间取决于从 DB 取回新号段的时间，并且这期间的业务请求会因为新号段没有取回来，导致线程阻塞。

我们可以在号段消耗到达某个值（比如10%）的时候，异步去数据库请求下一个号段，并加载到内存中。当前号段全部用完，如果下个号段准备好了，则切换到下个号段为当前号段接着下发 ID，循环反复。

![img](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2017/f2625fac.png)

- 每个 biz_tag 都有消费速度监控，推荐 segment 长度设置为服务高发期 QPS 的 600倍（10分钟），即使 DB 宕机，服务也能维持 10~20 分钟
- 每次请求来临，判断下个号段状态，从而更新新号段，偶尔的网络抖动不会影响下个号段的更新

**Leaf 高可用容灾**

对于 “DB可用性”问题，采用一主两从的方式，同时分机房部署，Master 和 Slave 之间采用半同步方式同步数据。同时 Leaf 服务分 IDC 部署，服务调用的时候，根据负载均衡算法会优先调用同机房的 Leaf 服务。

##### Leaf-snowflake 方案

Leaf-segment 方案可以生成趋势递增的 ID，同时 ID 号是可计算的，不适用于订单 ID 生成场景，竞争对手在两天中午分别下单，通过订单 ID 号相减就能大致计算出公司一天的订单量。

Leaf-snowflake 完全沿用 snowflake 方案的 bit 位设计，即 "1+41+10+12" 的方式组装 ID 号。对于 workerID 的分配，当服务集群数量较小的情况下，手动配置。Leaf 服务规模较大，使用 ZooKeeper 持久顺序节点的特性自动分配 workerID。步骤：

1. 启动 Leaf-snowflake 服务，连接 Zookeeper，在 leaf_forever 父节点下检查自己是否已经注册过
2. 注册过直接取回自己的 workerID，启动服务
3. 未注册过，在该父节点下创建一个持久顺序节点，创建成功后取回顺序号当做自己的 workerID，启动服务

**弱依赖 Zookeeper**

除了每次去 ZK 拿数据，workerID 也会缓存在本机文件系统上。当 ZK 出现问题，机器也要重启时，能保证服务正常启动，一定程度上提高了 SLA。

**解决时钟问题**

雪花算法依赖时间，如果机器的时钟发生了回拨，就会可能生成重复的 ID 号，需要解决时钟回退问题。

1. 在上一步中取得 workerID 之后，如果写过 zk 的 leaf_forever 节点，则用系统时间与 leaf_forever/${self} 节点记录时间做比较，若小于 leaf_forever/${self} 则认为机器时间发生了大步长回拨，服务启动失败并报警
2. 若未写过，证明是新服务节点，直接创建 leaf_forever/${self} 并写入自身系统时间。取 leaf_temporary 下所有临时节点（运行中的 Leaf-snowflake 节点）的服务 IP:Port，通过 RPC 请求得到所有节点的系统时间，计算平均值
3. 如果 abs(系统时间 - 节点时间平均值) < 阈值，认为当前系统时间准确，正常启动服务，同时写临时节点 leaf_temporary/${self} 维持租约
4. 否则认为本机系统时间发生大步长偏移，启动失败并报警
5. 每隔一段时间（3s）上报自身系统时间写入 leaf_forever/${self}

机器工作时 NTP 同步也会造成秒级回退，建议直接关闭 NTP 同步。可以在时钟回拨的时候直接不提供服务，返回 ERROR_CODE，等时钟追上。

或者做一层重试，然后上报报警系统。更或者是发现有时钟回拨之后自动摘除本身节点并报警。

```java
// 发生回拨，此刻时间小于上次发号时间
if (timestamp < lastTimestamp) {
    long offset = lastTimestamp - timestamp;
    if (offset <= 5) {
        try {
            wari(offset << 1); // 等待两倍时间
            timestamp = timeGen();
            if (timestamp < lastTimestamp) {
                throw ClockBackwardsException(timestamp);
            }
        } catch (InterruptedException e) {
            throw e;
        }
    } else {
        throw ClockBackwardsException(timestamp);
    }
}
// 分配 ID
```

### 场景

#### 一、订单系统

#### 1、一码付

一个二维码可以使用支付宝或者微信进行扫码支付。

二维码的本质是一个字符串，而聚合码的本质就是一个链接地址。用户使用支付宝微信直接扫一个码付钱，不用担心扫错了码，这极大减少了用户扫码支付的时间。

原理：当客户用 APP 扫码后，网站后台就会判断客户的扫码环境。（微信、支付宝、QQ 钱包、京东支付、云闪付等）。

判断环境的根据是打开链接浏览器的 HTTP header，header 中会有 User-Agent（UA）信息。

根据 UA，服务器可以识别出客户使用的操作系统及版本、CPU 类型，浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。

动态生成一码付的二维码预先绑定用户所选的商品信息和价格，根据用户所选的商品动态更新。用户扫码后，结合商品信息和价格、用户 UID，扫码环境，生成订单发送到第三方，第三方生成支付订单推送给用户设备，从而完成支付。

#### 2、订单号

订单号作为一个订单的唯一标识码，一般实现的业务场景有：

1. 用户订单遇到问题需要找客服进行协助；
2. 对订单进行操作，如线下收款，订单核销；
3. 下单、改单、成单、退单、售后等系统内部的订单流程处理和跟进

除了 ID 必要的特性外，订单号的设计需要体现几个特性：

##### 1、信息安全

编号不能透露公司的运营情况，比如销量、公司流水号等，以及商业信息和用户手机号、身份证等隐私信息。不能有明显的整体规律，防止修改一个字符就能查询到另一个订单信息。

##### 2、部分可读

位数要便于操作，因此订单号的位数要适中，且局部有规律。这样可以方便客服进行售后。

过长的订单号或者类似字母数字混合的可读性较差的订单号，会导致护肤输入困难，易错率较高。实际的业务场景中，订单号的设计通常会适当携带一些允许公开的、对使用场景有帮助的信息，比如下单时间、星期、类型等等。

##### 3、查询效率

int 类型相对 varchar 类型的查询效率更高（MySQL数据结构 B树有关 ）。

#### 3、优惠券和兑换券

常见场景：

1. 购买商品开通会员赠送的兑换码
2. 地方政府发放的消费券
3. 饮料上的兑奖码

有些 ID 适合即时生成，比如优惠券，用户领取的时候再分配 ID 即可；有些 ID 有线下的场景，需要预先生成。预先生成的券码具备的特性：

1. 预先生成，活动开始前就生成，进行活动预热；
2. 体量大，以万为单位，通常在 10 万级别以上；
3. 不可破解、仿制；
4. 支持用后核销（一次性）
5. 不一定被使用，利用率低，不适合使用数据库存储（占空间，有效数据少）

券码的生成，大多都是：将数字的二进制位分别利用起来表示不同的信息，然后编码成固定位数的字符。如：

优惠方案 ID + 兑换码序列号（自增）+ 校验码

具体：

1. 序列号 i，代表了当前优惠活动可以发行的兑换码数目，采用 30 bit表示，范围：1073741824（10亿）
2. 优惠方案 ID，代表当前优惠方案的 ID 号，决定了可以组织的优惠活动的次数，采用15 bit 表示，范围：32768
3. 校验码，检验兑换码是否有效，快捷地校验兑换码信息是否正确，还可以填充数据，增强数据散列性，使用13位表示。

所有位数组合后转码为 a-zA-Z0-9 即可。

#### 二、Tracing

在分布式服务架构下，一个 Web 请求从网关流入，有可能会调用多个服务对请求进行处理，拿到最终结果。这个过程中每个服务之间的通信又是单独的网络请求，无论请求经过的哪个服务出了故障或者处理过慢都会对前端造成影响。

处理一个 Web 请求要调用的多个服务，为了能更方便的查询哪个环节的服务出现了问题，现在常用的解决方案是为整个系统引入分布式链路跟踪。

在整个请求的调用链中，请求会一直携带 traceid 往下游服务传递，每个服务内部也会生成自己的 spanid 用于生成自己的内部调用视图，并和 traceid 一起传递给下游服务。

##### TraceId 生成规则

生成的 ID 除了要求唯一之外，还要求生成的效率高、吞吐量大。traceid 需要具备接入层的服务器实例自主生成的能力，如果每个 trace 中的 ID 都需要请求公共的 ID 服务生成，纯纯的浪费网络带宽资源。且会阻塞用户请求向下游传递，响应耗时上升，增加了没必要的风险。所以需要服务器实例最好可以自行计算 tracid，spanid，避免依赖外部服务。

产生规则：服务器 IP + ID 产生的时间 + 自增序列 + 当前进程号 ，比如：

0ad1348f1403169275002100356696

前 8 位 0ad1348f 即产生 TraceId 的机器的 IP，这是一个十六进制的数字，每两位代表 IP 中的一段，我们把这个数字，按每两位转成 10 进制即可得到常见的 IP 地址表示方式 10.209.52.143，您也可以根据这个规律来查找到请求经过的第一个服务器。

后面的 13 位 1403169275002 是产生 TraceId 的时间。之后的 4 位 1003 是一个自增的序列，从 1000 涨到 9000，到达 9000 后回到 1000 再开始往上涨。最后的 5 位 56696 是当前的进程 ID，为了防止单机多进程出现 TraceId 冲突的情况，所以在 TraceId 末尾添加了当前的进程 ID。

##### SpanId 生成规则

span 是层的意思，比如在第一个实例算是第一层， 请求代理或者分流到下一个实例处理，就是第二层，以此类推。通过层，SpanId 代表本次调用在整个调用链路树中的位置。

假设一个 服务器实例 A 接收了一次用户请求，代表是整个调用的根节点，那么 A 层处理这次请求产生的非服务调用日志记录 spanid 的值都是 0，A 层需要通过 RPC 依次调用 B、C、D 三个服务器实例，那么在 A 的日志中，SpanId 分别是 0.1，0.2 和 0.3，在 B、C、D 中，SpanId 也分别是 0.1，0.2 和 0.3；如果 C 系统在处理请求的时候又调用了 E，F 两个服务器实例，那么 C 系统中对应的 spanid 是 0.2.1 和 0.2.2，E、F 两个系统对应的日志也是 0.2.1 和 0.2.2。

根据上面的描述可以知道，如果把一次调用中所有的 SpanId 收集起来，可以组成一棵完整的链路树。

**spanid 的生成本质：在跨层传递透传的同时，控制大小版本号的自增来实现的。**

#### 三、短网址

常用的 ID 生成服务比如：MySQL ID 自增、 Redis 键自增、号段模式，生成的 ID 都是一串数字。短网址服务把客户的长网址转换成短网址，

实际是在 **dwz.cn** 域名后面拼接新产生的数字类型 ID，直接用数字 ID，网址长度也有些长，服务可以通过数字 ID 转更高进制的方式压缩长度。这种算法在短网址的技术实现上越来越多了起来，它可以进一步压缩网址长度。转进制的压缩算法在生活中有广泛的应用场景。

原理：将数字转为62进制。

